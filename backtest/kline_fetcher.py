"""
K ç·šç²å–å™¨ - éåŒæ­¥ä¸¦è¡Œç²å–å’Œç·©å­˜ (æ”¯æŒæ™ºèƒ½å¢é‡æ›´æ–°)
ä½¿ç”¨ asyncio + aiohttp å¯¦ç¾ 20x æ€§èƒ½åŠ é€Ÿ
æ”¯æŒ Binance å’Œ Bybit çš„åˆ†é ç²å–
æ ¹æ“šå¿«å–è¦†è“‹å’Œ symbol ä¸Šå¸‚æ™‚é–“æ™ºèƒ½è£œå……ç¼ºå¤±æ•¸æ“š
"""

import asyncio
import aiohttp
import pandas as pd
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
import time
from tqdm import tqdm
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from data_collector.utils import standardize_timestamp_column, validate_timestamp_range

logger = logging.getLogger(__name__)


class KlineFetcher:
    """K ç·šéåŒæ­¥ç²å–å™¨ (æ”¯æŒæ™ºèƒ½å¢é‡æ›´æ–°)"""
    
    def __init__(self, cache_dir: str = "/tmp/kline_cache"):
        """
        åˆå§‹åŒ–ç²å–å™¨
        
        Args:
            cache_dir: K ç·šå¿«å–ç›®éŒ„
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Semaphore è¨­å®š (é™åˆ¶ä¸¦ç™¼åº¦)
        self.bn_semaphore = asyncio.Semaphore(5)   # Binance: 5 ä¸¦ç™¼
        self.by_semaphore = asyncio.Semaphore(5)   # Bybit: 5 ä¸¦ç™¼
        
        # API é…ç½®
        self.BINANCE_LIMIT = 1000  # Binance æ¯æ¬¡æœ€å¤šè¿”å› 1000 æ¢
        self.BYBIT_LIMIT = 200     # Bybit æ¯æ¬¡æœ€å¤šè¿”å› 200 æ¢
        
        # Symbol ä¸Šå¸‚æ™‚é–“å¿«å–
        self.listing_times: Dict[str, Dict[str, Optional[int]]] = {}
    
    def _get_cache_path(self, symbol: str, exchange: str) -> Path:
        """ç”Ÿæˆå¿«å–è·¯å¾‘ (ç°¡åŒ–æ ¼å¼)"""
        return self.cache_dir / f"{symbol}_{exchange}.parquet"
    
    def _check_cache_coverage(
        self,
        symbol: str,
        exchange: str,
        required_start_ms: int,
        required_end_ms: int
    ) -> Tuple[bool, Optional[pd.DataFrame]]:
        """
        æª¢æŸ¥å¿«å–è¦†è“‹æƒ…æ³
        
        Returns:
            (is_complete, dataframe)
            - is_complete: True è¡¨ç¤ºå¿«å–å®Œå…¨è¦†è“‹æ‰€éœ€æ™‚é–“ç¯„åœ
            - dataframe: å¿«å–æ•¸æ“š (å¦‚æœå­˜åœ¨)
        """
        cache_path = self._get_cache_path(symbol, exchange)
        
        if not cache_path.exists():
            return False, None
        
        try:
            df = pd.read_parquet(cache_path)
            
            if len(df) == 0:
                return False, None
            
            # ä½¿ç”¨çµ±ä¸€çš„æ™‚é–“æˆ³æ¨™æº–åŒ–å‡½æ•¸
            df = standardize_timestamp_column(df, col='timestamp')
            
            # é©—è­‰æ™‚é–“æˆ³ç¯„åœ
            if not validate_timestamp_range(df['timestamp'].values):
                logger.warning(f"å¿«å–æ™‚é–“æˆ³è¶…å‡ºç¯„åœ {symbol}({exchange})")
                return False, None
            
            # æª¢æŸ¥æ™‚é–“ç¯„åœ
            cache_start = int(df['timestamp'].min())
            cache_end = int(df['timestamp'].max())
            
            # æª¢æŸ¥æ˜¯å¦å®Œå…¨è¦†è“‹
            is_complete = (cache_start <= required_start_ms and 
                          cache_end >= required_end_ms)
            
            if is_complete:
                logger.debug(f"å¿«å–å®Œå…¨è¦†è“‹ {symbol}({exchange}): "
                           f"{cache_start} ~ {cache_end}")
            else:
                logger.debug(f"å¿«å–éƒ¨åˆ†è¦†è“‹ {symbol}({exchange}): "
                           f"å¿«å– {cache_start} ~ {cache_end}, "
                           f"éœ€è¦ {required_start_ms} ~ {required_end_ms}")
            
            return is_complete, df
        except Exception as e:
            logger.warning(f"å¿«å–è®€å–å¤±æ•— {symbol}({exchange}): {e}")
            return False, None
    
    def _normalize_listing_time(self, listing_time_ms: Optional[int]) -> Optional[int]:
        """
        è¦ç¯„åŒ–ä¸Šå¸‚æ™‚é–“ï¼Œç¢ºä¿å–®ä½æ­£ç¢º
        ä¸Šå¸‚æ™‚é–“æ‡‰è©²åœ¨ 2017-2025 å¹´ä¹‹é–“ï¼ˆåˆç†ç¯„åœï¼‰
        
        Args:
            listing_time_ms: å¯èƒ½ä¾†è‡ª API çš„ä¸Šå¸‚æ™‚é–“
        
        Returns:
            è¦ç¯„åŒ–å¾Œçš„æ¯«ç§’æ™‚é–“æˆ³ï¼Œæˆ– None å¦‚æœç„¡æ•ˆ
        """
        if listing_time_ms is None:
            return None
        
        # åˆç†çš„æ™‚é–“ç¯„åœï¼š2017-01-01 ~ 2025-12-31 (æ¯«ç§’)
        MIN_TIME_MS = 1483228800000  # 2017-01-01
        MAX_TIME_MS = 1767225599999  # 2025-12-31
        
        # æª¢æŸ¥æ˜¯å¦åœ¨åˆç†ç¯„åœ
        if MIN_TIME_MS <= listing_time_ms <= MAX_TIME_MS:
            return listing_time_ms
        
        # å¦‚æœå¤ªå¤§ï¼Œå¯èƒ½æ˜¯ç§’è€Œä¸æ˜¯æ¯«ç§’
        if listing_time_ms > MAX_TIME_MS:
            listing_time_seconds = listing_time_ms // 1000
            if MIN_TIME_MS <= listing_time_seconds * 1000 <= MAX_TIME_MS:
                # å¯èƒ½æ˜¯ä»¥ç§’ç‚ºå–®ä½ï¼Œè½‰æ›ç‚ºæ¯«ç§’
                logger.debug(f"ä¸Šå¸‚æ™‚é–“å¯èƒ½ä»¥ç§’ç‚ºå–®ä½ï¼Œè½‰æ›: {listing_time_ms} â†’ {listing_time_seconds * 1000}")
                return listing_time_seconds * 1000
        
        # ç„¡æ³•è¦ç¯„åŒ–ï¼Œè¿”å› None
        logger.warning(f"ä¸Šå¸‚æ™‚é–“è¶…å‡ºåˆç†ç¯„åœï¼Œå¿½ç•¥: {listing_time_ms}")
        return None
    
    def _calculate_missing_periods(
        self,
        symbol: str,
        exchange: str,
        required_start_ms: int,
        required_end_ms: int,
        cached_start_ms: Optional[int],
        cached_end_ms: Optional[int]
    ) -> List[Tuple[int, int, str]]:
        """
        è¨ˆç®—ç¼ºå¤±çš„æ™‚é–“æ®µ
        æ ¹æ“šå¿«å–è¦†è“‹å’Œ symbol ä¸Šå¸‚æ™‚é–“æ™ºèƒ½æ±ºå®šéœ€è¦è£œå……çš„æ•¸æ“š
        
        é‚è¼¯ï¼š
        1. ä¸æŠ“å– symbol ä¸Šå¸‚å‰çš„æ•¸æ“š
        2. åªè£œå……å¿«å–æ²’æœ‰è¦†è“‹çš„æ™‚é–“æ®µ
        3. å¦‚æœæ²’æœ‰å¿«å–ï¼ŒæŠ“å–å®Œæ•´æ™‚é–“ç¯„åœ
        
        Args:
            symbol: äº¤æ˜“å°
            exchange: äº¤æ˜“æ‰€
            required_start_ms: æ‰€éœ€é–‹å§‹æ™‚é–“
            required_end_ms: æ‰€éœ€çµæŸæ™‚é–“
            cached_start_ms: å¿«å–é–‹å§‹æ™‚é–“ (None è¡¨ç¤ºç„¡å¿«å–)
            cached_end_ms: å¿«å–çµæŸæ™‚é–“
        
        Returns:
            [(start_ms, end_ms, period_type), ...]
            period_type: 'before', 'after', æˆ– 'full'
        """
        fetch_periods = []
        
        # ç²å–è©² symbol åœ¨æ­¤äº¤æ˜“æ‰€çš„ä¸Šå¸‚æ™‚é–“
        listing_time_ms = None
        if symbol in self.listing_times:
            listing_time_ms = self.listing_times[symbol].get(exchange)
        
        # æ±ºå®šå¯¦éš›çš„é–‹å§‹æ™‚é–“ (ä¸æ—©æ–¼ä¸Šå¸‚æ™‚é–“)
        effective_start_ms = required_start_ms
        if listing_time_ms is not None and required_start_ms < listing_time_ms:
            logger.debug(f"[Cache] {symbol}({exchange}): gap start before listing time, "
                        f"adjusted fetch start from {datetime.fromtimestamp(required_start_ms/1000).strftime('%Y-%m-%d')} "
                        f"to {datetime.fromtimestamp(listing_time_ms/1000).strftime('%Y-%m-%d')}")
            effective_start_ms = listing_time_ms
        elif listing_time_ms is None:
            # æ²’æœ‰ä¸Šå¸‚æ™‚é–“ä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹æ™‚é–“
            logger.debug(f"[Cache] {symbol}({exchange}): no listing time available, using required start {datetime.fromtimestamp(required_start_ms/1000).strftime('%Y-%m-%d')}")
        else:
            # ä¸Šå¸‚æ™‚é–“åœ¨æ‰€éœ€æ™‚é–“ä¹‹å¾Œï¼Œæ­£å¸¸ä½¿ç”¨æ‰€éœ€æ™‚é–“
            logger.debug(f"[Cache] {symbol}({exchange}): listing time {datetime.fromtimestamp(listing_time_ms/1000).strftime('%Y-%m-%d')} is after required start {datetime.fromtimestamp(required_start_ms/1000).strftime('%Y-%m-%d')}")
        
        # ç„¡å¿«å–æƒ…æ³
        if cached_start_ms is None:
            logger.info(f"[Cache] {symbol}({exchange}): no cache, fetching full period ({datetime.fromtimestamp(effective_start_ms/1000).strftime('%Y-%m-%d')} ~ {datetime.fromtimestamp(required_end_ms/1000).strftime('%Y-%m-%d')})")
            fetch_periods.append((effective_start_ms, required_end_ms, "full"))
            return fetch_periods
        
        # å‰æ®µç¼ºå¤± (effective_start < cached_start)
        if effective_start_ms < cached_start_ms:
            gap_before_ms = cached_start_ms - effective_start_ms
            gap_before_days = gap_before_ms / (24 * 60 * 60 * 1000)
            
            # æª¢æŸ¥æ˜¯å¦å¿«å–å‰çš„ç¼ºå¤±çµæŸæ–¼ä¸Šå¸‚æ™‚é–“ä¹‹å‰
            if listing_time_ms is not None and gap_before_ms > 0:
                if effective_start_ms <= listing_time_ms <= cached_start_ms:
                    logger.info(f"[Cache] {symbol}({exchange}): gap before cache ends before listing time "
                               f"({datetime.fromtimestamp(listing_time_ms/1000).strftime('%Y-%m-%d')}), SKIPPING fetch")
                    # ä¸è£œå……ä¸Šå¸‚æ™‚é–“ä¹‹å‰çš„æ•¸æ“š
                else:
                    logger.info(f"[Cache] {symbol}({exchange}): gap before cache ({gap_before_days:.1f} days), "
                               f"fetching from {datetime.fromtimestamp(effective_start_ms/1000).strftime('%Y-%m-%d')}...")
                    fetch_periods.append((effective_start_ms, cached_start_ms, "before"))
            else:
                logger.info(f"[Cache] {symbol}({exchange}): gap before cache ({gap_before_days:.1f} days), "
                           f"fetching from {datetime.fromtimestamp(effective_start_ms/1000).strftime('%Y-%m-%d')}...")
                fetch_periods.append((effective_start_ms, cached_start_ms, "before"))
        
        # å¾Œæ®µç¼ºå¤± (required_end > cached_end)
        if required_end_ms > cached_end_ms:
            gap_after_ms = required_end_ms - cached_end_ms
            gap_after_days = gap_after_ms / (24 * 60 * 60 * 1000)
            
            # å¦‚æœç¼ºå¤±å°‘æ–¼ 2 åˆ†é˜ï¼ˆ120,000 æ¯«ç§’ï¼‰ï¼Œèªç‚ºæ˜¯æ™‚é–“æˆ³ç²¾åº¦èª¤å·®ï¼Œä¸è£œå……
            MIN_GAP_MS = 120000  # 2 åˆ†é˜
            
            if gap_after_ms < MIN_GAP_MS:
                logger.debug(f"[Cache] {symbol}({exchange}): gap after cache å¤ªå° ({gap_after_ms} ms), è·³éè£œå……")
            else:
                logger.info(f"[Cache] {symbol}({exchange}): gap after cache ({gap_after_days:.1f} days), fetching...")
                fetch_periods.append((cached_end_ms, required_end_ms, "after"))
        
        return fetch_periods
    
    async def fetch_klines_async(
        self,
        symbol: str,
        exchange: str,
        start_ms: int,
        end_ms: int,
        session: Optional[aiohttp.ClientSession] = None,
        max_retries: int = 3
    ) -> Optional[pd.DataFrame]:
        """
        éåŒæ­¥ç²å– K ç·š (æ”¯æŒæ™ºèƒ½å¢é‡æ›´æ–°)
        æ ¹æ“šå¿«å–è¦†è“‹å’Œ symbol ä¸Šå¸‚æ™‚é–“æ±ºå®šæ˜¯å¦éœ€è¦è£œå……æ•¸æ“š
        
        Args:
            symbol: äº¤æ˜“å°
            exchange: äº¤æ˜“æ‰€ ('binance' or 'bybit')
            start_ms: é–‹å§‹æ™‚é–“æˆ³ (æ¯«ç§’)
            end_ms: çµæŸæ™‚é–“æˆ³ (æ¯«ç§’)
            session: aiohttp æœƒè©± (å¯é¸)
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        
        Returns:
            K ç·š DataFrame æˆ– None
        """
        # 1. æª¢æŸ¥å¿«å–è¦†è“‹
        is_complete, cached_df = self._check_cache_coverage(
            symbol, exchange, start_ms, end_ms
        )
        
        if is_complete and cached_df is not None:
            # å¿«å–å®Œå…¨è¦†è“‹ï¼Œç›´æ¥è¿”å›æ‰€éœ€ç¯„åœçš„æ•¸æ“š
            logger.debug(f"[Cache] {symbol}({exchange}): ä½¿ç”¨å®Œæ•´å¿«å–")
            # ç¢ºä¿ timestamp æ˜¯æ¯«ç§’æ•´æ•¸
            result_df = cached_df[
                (cached_df['timestamp'] >= start_ms) &
                (cached_df['timestamp'] <= end_ms)
            ].reset_index(drop=True)
            
            # è½‰æ› timestamp ç‚ºæ¯«ç§’æ•´æ•¸ï¼ˆå¦‚æœé‚„ä¸æ˜¯ï¼‰
            if len(result_df) > 0 and result_df['timestamp'].dtype != 'int64':
                result_df['timestamp'] = result_df['timestamp'].astype('int64')
            
            # ç¢ºä¿ OHLCV æ¬„ä½éƒ½æ˜¯ float64
            if len(result_df) > 0:
                for col in ['open', 'high', 'low', 'close', 'volume']:
                    if col in result_df.columns and result_df[col].dtype != 'float64':
                        result_df[col] = pd.to_numeric(result_df[col], errors='coerce').astype('float64')
            
            return result_df
        
        # 2. è¨ˆç®—ç¼ºå¤±çš„æ™‚é–“æ®µ
        cached_start_ms = None
        cached_end_ms = None
        if cached_df is not None and len(cached_df) > 0:
            # æ™‚é–“æˆ³å·²ç¶“åœ¨ _check_cache_coverage ä¸­æ¨™æº–åŒ–ç‚ºæ¯«ç§’æ•´æ•¸
            ts_min = int(cached_df['timestamp'].min())
            ts_max = int(cached_df['timestamp'].max())
            
            cached_start_ms = ts_min
            cached_end_ms = ts_max
        
        fetch_periods = self._calculate_missing_periods(
            symbol, exchange, start_ms, end_ms, cached_start_ms, cached_end_ms
        )
        
        # 3. æŠ“å–ç¼ºå¤±çš„æ™‚é–“æ®µ
        all_new_data = []
        for period_start, period_end, period_type in fetch_periods:
            try:
                start_date = datetime.fromtimestamp(period_start/1000).strftime('%Y-%m-%d')
                end_date = datetime.fromtimestamp(period_end/1000).strftime('%Y-%m-%d')
                logger.info(f"[Fetch] {symbol}({exchange}): fetching {period_type} period ({start_date} ~ {end_date})")
            except (ValueError, OSError):
                logger.info(f"[Fetch] {symbol}({exchange}): fetching {period_type} period")
            
            for attempt in range(max_retries):
                try:
                    if exchange == 'binance':
                        df = await self._fetch_binance_klines_paginated(
                            symbol, period_start, period_end, session
                        )
                    elif exchange == 'bybit':
                        df = await self._fetch_bybit_klines_paginated(
                            symbol, period_start, period_end, session
                        )
                    else:
                        df = None
                    
                    if df is not None and len(df) > 0:
                        all_new_data.append(df)
                        logger.info(f"[Fetch] {symbol}({exchange}): fetched {len(df)} records "
                                  f"for {period_type} period")
                        break
                    else:
                        logger.warning(f"[Fetch] {symbol}({exchange}): no data for {period_type} period")
                        break
                        
                except Exception as e:
                    logger.warning(f"[Fetch] {symbol}({exchange}): failed (attempt {attempt+1}/{max_retries}): {e}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
        
        # 4. åˆä½µå¿«å–å’Œæ–°æ•¸æ“š
        if all_new_data:
            new_df = pd.concat(all_new_data, ignore_index=True)
        else:
            new_df = None
        
        if new_df is not None and cached_df is not None and len(cached_df) > 0:
            # åˆä½µå¿«å–å’Œæ–°æ•¸æ“š
            combined_df = pd.concat([cached_df, new_df], ignore_index=True)
            # ç§»é™¤é‡è¤‡ (æŒ‰ timestamp)
            combined_df = combined_df.drop_duplicates(subset=['timestamp'], keep='first')
            combined_df = combined_df.sort_values('timestamp').reset_index(drop=True)
        elif new_df is not None:
            combined_df = new_df.sort_values('timestamp').reset_index(drop=True)
        elif cached_df is not None:
            combined_df = cached_df
        else:
            combined_df = None
        
        # 5. ä¿å­˜æ›´æ–°çš„å¿«å–
        if combined_df is not None and len(combined_df) > 0:
            # ç¢ºä¿æ™‚é–“æˆ³å·²æ¨™æº–åŒ–ç‚ºæ¯«ç§’æ•´æ•¸
            combined_df = standardize_timestamp_column(combined_df, col='timestamp')
            
            # ç¢ºä¿ OHLCV æ¬„ä½éƒ½æ˜¯ float64
            for col in ['open', 'high', 'low', 'close', 'volume']:
                if col in combined_df.columns:
                    combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce').astype('float64')
            
            try:
                cache_path = self._get_cache_path(symbol, exchange)
                combined_df.to_parquet(cache_path, index=False, compression='snappy')
                logger.debug(f"[Cache] {symbol}({exchange}): ä¿å­˜å¿«å– {cache_path.name} ({len(combined_df)} è¡Œ)")
            except Exception as e:
                logger.warning(f"[Cache] {symbol}({exchange}): å¿«å–ä¿å­˜å¤±æ•—: {e}")
            
            # è¿”å›æ‰€éœ€ç¯„åœçš„æ•¸æ“š
            result_df = combined_df[
                (combined_df['timestamp'] >= start_ms) &
                (combined_df['timestamp'] <= end_ms)
            ].reset_index(drop=True)
            
            # æœ€çµ‚é©—è­‰æ™‚é–“æˆ³ç¯„åœ
            if len(result_df) > 0 and not validate_timestamp_range(result_df['timestamp'].values):
                logger.warning(f"è¿”å›çš„ K ç·šæ•¸æ“šæ™‚é–“æˆ³è¶…å‡ºç¯„åœ {symbol}({exchange})")
            
            # ç¢ºä¿ OHLCV æ¬„ä½éƒ½æ˜¯ float64
            if len(result_df) > 0:
                for col in ['open', 'high', 'low', 'close', 'volume']:
                    if col in result_df.columns and result_df[col].dtype != 'float64':
                        result_df[col] = pd.to_numeric(result_df[col], errors='coerce').astype('float64')
            
            return result_df
        
        return None
    
    async def _fetch_binance_klines_paginated(
        self,
        symbol: str,
        start_ms: int,
        end_ms: int,
        session: Optional[aiohttp.ClientSession] = None
    ) -> Optional[pd.DataFrame]:
        """
        Binance K ç·šåˆ†é ç²å– (å¾èˆŠåˆ°æ–°)
        """
        url = "https://api.binance.com/api/v3/klines"
        all_data = []
        current_start = start_ms
        
        while current_start < end_ms:
            try:
                params = {
                    'symbol': symbol,
                    'interval': '1m',
                    'startTime': int(current_start),
                    'endTime': int(end_ms),
                    'limit': self.BINANCE_LIMIT
                }
                
                if session is None:
                    async with aiohttp.ClientSession() as s:
                        async with s.get(url, params=params, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                            if resp.status == 200:
                                data = await resp.json()
                            else:
                                # è™•ç†ä¸åŒçš„ HTTP ç‹€æ…‹ç¢¼
                                if resp.status == 400:
                                    # 400 Bad Request - å¯èƒ½æ˜¯ç¬¦è™Ÿç„¡æ•ˆæˆ–ä¸å­˜åœ¨
                                    error_msg = await resp.text()
                                    logger.warning(f"Binance API 400 - Symbol å¯èƒ½ç„¡æ•ˆæˆ–ä¸å­˜åœ¨: {symbol} ({error_msg[:100]})")
                                    break  # ç›´æ¥è·³éï¼Œä¸é‡è©¦
                                elif resp.status in (418, 429):
                                    # 418 I'm a teapot (speed limit) æˆ– 429 Too Many Requests
                                    logger.error(f"Binance rate limit (HTTP {resp.status})ï¼Œæš«åœ 60 ç§’...")
                                    await asyncio.sleep(60)
                                    break  # æš«åœå¾Œæ”¾æ£„é€™å€‹è«‹æ±‚
                                else:
                                    logger.warning(f"Binance API éŒ¯èª¤ HTTP {resp.status}")
                                    break
                else:
                    async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                        else:
                            # è™•ç†ä¸åŒçš„ HTTP ç‹€æ…‹ç¢¼
                            if resp.status == 400:
                                # 400 Bad Request - å¯èƒ½æ˜¯ç¬¦è™Ÿç„¡æ•ˆæˆ–ä¸å­˜åœ¨
                                error_msg = await resp.text()
                                logger.warning(f"Binance API 400 - Symbol å¯èƒ½ç„¡æ•ˆæˆ–ä¸å­˜åœ¨: {symbol} ({error_msg[:100]})")
                                break  # ç›´æ¥è·³éï¼Œä¸é‡è©¦
                            elif resp.status in (418, 429):
                                # 418 I'm a teapot (speed limit) æˆ– 429 Too Many Requests
                                logger.error(f"Binance rate limit (HTTP {resp.status})ï¼Œæš«åœ 60 ç§’...")
                                await asyncio.sleep(60)
                                break  # æš«åœå¾Œæ”¾æ£„é€™å€‹è«‹æ±‚
                            else:
                                logger.warning(f"Binance API éŒ¯èª¤ HTTP {resp.status}")
                                break
                
                if not data:
                    logger.debug(f"{symbol} ç„¡æ›´å¤šæ•¸æ“š")
                    break
                
                all_data.extend(data)
                
                # æ›´æ–°ä¸‹ä¸€æ‰¹çš„é–‹å§‹æ™‚é–“
                last_timestamp = data[-1][0]
                current_start = last_timestamp + 1
                
                # é¿å… rate limit
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.warning(f"Binance å–®æ¬¡è«‹æ±‚å¤±æ•—: {e}")
                break
        
        if not all_data:
            return None
        
        # è½‰æ›ç‚º DataFrame
        df = pd.DataFrame(all_data, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_asset_volume', 'number_of_trades',
            'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
        ])
        
        # è½‰æ›æ•¸æ“šé¡å‹
        df['timestamp'] = df['timestamp'].astype('int64')
        df['open'] = df['open'].astype('float64')
        df['high'] = df['high'].astype('float64')
        df['low'] = df['low'].astype('float64')
        df['close'] = df['close'].astype('float64')
        df['volume'] = df['volume'].astype('float64')
        
        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]
        
        return df
    
    async def _fetch_bybit_klines_paginated(
        self,
        symbol: str,
        start_ms: int,
        end_ms: int,
        session: Optional[aiohttp.ClientSession] = None
    ) -> Optional[pd.DataFrame]:
        """
        Bybit K ç·šåˆ†é ç²å– (å¾æ–°åˆ°èˆŠï¼Œéœ€è¦åè½‰)
        """
        url = "https://api.bybit.com/v5/market/kline"
        all_data = []
        current_end = end_ms  # å¾æœ€æ–°æ™‚é–“é–‹å§‹å¾€å›
        
        while current_end > start_ms:
            try:
                params = {
                    'category': 'linear',
                    'symbol': symbol,
                    'interval': '1',
                    'end': str(int(current_end)),
                    'limit': str(self.BYBIT_LIMIT)
                }
                
                if session is None:
                    async with aiohttp.ClientSession() as s:
                        async with s.get(url, params=params, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                            if resp.status == 200:
                                result = await resp.json()
                                data = result.get('result', {}).get('list', [])
                            else:
                                logger.warning(f"Bybit API éŒ¯èª¤: {resp.status}")
                                if resp.status == 429:
                                    logger.error(f"å·²é” Bybit rate limitï¼Œæš«åœ 30 ç§’...")
                                    await asyncio.sleep(30)
                                break
                else:
                    async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=30)) as resp:
                        if resp.status == 200:
                            result = await resp.json()
                            data = result.get('result', {}).get('list', [])
                        else:
                            logger.warning(f"Bybit API éŒ¯èª¤: {resp.status}")
                            if resp.status == 429:
                                logger.error(f"å·²é” Bybit rate limitï¼Œæš«åœ 30 ç§’...")
                                await asyncio.sleep(30)
                            break
                
                if not data:
                    logger.debug(f"{symbol} ç„¡æ›´å¤šæ•¸æ“š")
                    break
                
                all_data.extend(data)
                
                # å¦‚æœå·²ç¶“å›åˆ° start_ms å°±åœæ­¢
                current_end = int(data[-1][0]) - 1
                if current_end <= start_ms:
                    break
                
                # é¿å… rate limit
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.warning(f"Bybit å–®æ¬¡è«‹æ±‚å¤±æ•—: {e}")
                break
        
        if not all_data:
            return None
        
        # è½‰æ›ç‚º DataFrame (Bybit è¿”å› 7 åˆ—)
        df = pd.DataFrame(all_data, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover'
        ])
        
        # è½‰æ›æ•¸æ“šé¡å‹
        df['timestamp'] = df['timestamp'].astype('int64')
        df['open'] = df['open'].astype('float64')
        df['high'] = df['high'].astype('float64')
        df['low'] = df['low'].astype('float64')
        df['close'] = df['close'].astype('float64')
        df['volume'] = df['volume'].astype('float64')
        
        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]
        
        # âš ï¸ Bybit è¿”å›æ–°â†’èˆŠæ’åºï¼Œéœ€è¦åè½‰ç‚ºèˆŠâ†’æ–°
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        return df
    
    async def _fetch_with_semaphore(
        self,
        symbol: str,
        exchange: str,
        start_ms: int,
        end_ms: int,
        session: aiohttp.ClientSession,
        semaphore: asyncio.Semaphore
    ) -> Tuple[str, str, Optional[pd.DataFrame]]:
        """å¸¶ Semaphore çš„ç²å– (æ§åˆ¶ä¸¦ç™¼)"""
        async with semaphore:
            df = await self.fetch_klines_async(
                symbol, exchange, start_ms, end_ms, session
            )
            return symbol, exchange, df
    
    async def _load_listing_times(
        self,
        tradable_symbols: List[str]
    ) -> None:
        """
        ç•°æ­¥åŠ è¼‰æ‰€æœ‰ symbols çš„ä¸Šå¸‚æ™‚é–“
        æ‡‰ç”¨èˆ‡ opportunity_analysis ç›¸åŒçš„é‚è¼¯ (ä½¿ç”¨ async with context manager)
        å¤±æ•—ä¸æœƒä¸­æ–·æµç¨‹ï¼Œåªæ˜¯ç„¡æ³•åˆ©ç”¨ä¸Šå¸‚æ™‚é–“å„ªåŒ–
        """
        logger.info(f"ğŸ“… [Phase 0] åŠ è¼‰ {len(tradable_symbols)} å€‹ symbols çš„ä¸Šå¸‚æ™‚é–“...")
        
        # åˆå§‹åŒ–ç‚ºç©º
        self.listing_times = {s: {'binance': None, 'bybit': None} for s in tradable_symbols}
        
        try:
            from data_collector.binance_client import BinanceClient
            from data_collector.bybit_client import BybitClient
            
            # ä½¿ç”¨ context manager ç¢ºä¿ session è¢«æ­£ç¢ºåˆå§‹åŒ–å’Œé—œé–‰ (èˆ‡ analysis ç›¸åŒ)
            async with BinanceClient() as bn_client, BybitClient() as by_client:
                
                # å˜—è©¦åŠ è¼‰ Binance ä¸Šå¸‚æ™‚é–“
                try:
                    bn_listing_times = await bn_client.get_all_symbols_listing_times()
                    if isinstance(bn_listing_times, dict):
                        for symbol, bn_time in bn_listing_times.items():
                            if symbol in self.listing_times:
                                self.listing_times[symbol]['binance'] = bn_time
                        logger.debug(f"âœ“ åŠ è¼‰ {len(bn_listing_times)} å€‹ Binance ä¸Šå¸‚æ™‚é–“")
                except Exception as e:
                    logger.warning(f"åŠ è¼‰ Binance ä¸Šå¸‚æ™‚é–“å¤±æ•—: {e}")
                
                # å˜—è©¦åŠ è¼‰ Bybit ä¸Šå¸‚æ™‚é–“ (é€å€‹è«‹æ±‚ï¼Œä½†éŒ¯èª¤ä¸ä¸­æ–·)
                try:
                    loaded_count = 0
                    failed_count = 0
                    no_data_count = 0
                    
                    # ç”¨ gather with return_exceptions é¿å…å–®å€‹å¤±æ•—å½±éŸ¿æ•´é«”
                    by_tasks = [by_client.get_symbol_listing_time(symbol) for symbol in tradable_symbols]
                    by_listing_times = await asyncio.gather(*by_tasks, return_exceptions=True)
                    
                    for symbol, by_time in zip(tradable_symbols, by_listing_times):
                        if isinstance(by_time, Exception):
                            # ç•°å¸¸ï¼Œè·³é
                            failed_count += 1
                            logger.debug(f"Bybit {symbol}: åŠ è¼‰å¤±æ•— ({type(by_time).__name__}: {by_time})")
                        elif by_time is not None:
                            self.listing_times[symbol]['bybit'] = by_time
                            loaded_count += 1
                        else:
                            no_data_count += 1
                            logger.debug(f"Bybit {symbol}: ç„¡ä¸Šå¸‚æ™‚é–“æ•¸æ“š (ä¸åœ¨ Bybit ä¸Šå¸‚)")
                    
                    logger.debug(f"âœ“ åŠ è¼‰ {loaded_count} å€‹ Bybit ä¸Šå¸‚æ™‚é–“ "
                               f"({failed_count} å€‹ç•°å¸¸, {no_data_count} å€‹ç„¡æ•¸æ“š)")
                    
                except Exception as e:
                    logger.warning(f"åŠ è¼‰ Bybit ä¸Šå¸‚æ™‚é–“å¤±æ•—: {e}")
            
            # çµ±è¨ˆæˆåŠŸåŠ è¼‰çš„æ•¸é‡
            bn_loaded = sum(1 for times in self.listing_times.values() if times.get('binance') is not None)
            by_loaded = sum(1 for times in self.listing_times.values() if times.get('bybit') is not None)
            both_loaded = sum(1 for times in self.listing_times.values() 
                            if times.get('binance') is not None and times.get('bybit') is not None)
            
            logger.info(f"âœ“ [Phase 0] ä¸Šå¸‚æ™‚é–“åŠ è¼‰å®Œæˆ")
            logger.info(f"   Binance: {bn_loaded}/{len(tradable_symbols)} âœ“")
            logger.info(f"   Bybit:   {by_loaded}/{len(tradable_symbols)} âœ“")
            logger.info(f"   é›™é‚Šéƒ½æœ‰: {both_loaded}/{len(tradable_symbols)}")
            
            # å¦‚æœæŸäº› symbols åªæœ‰å–®é‚Šä¸Šå¸‚æ™‚é–“ï¼Œæ—¥èªŒæœƒé¡¯ç¤º
            if bn_loaded > 0 or by_loaded > 0:
                logger.debug(f"   å°‡ä½¿ç”¨ä¸Šå¸‚æ™‚é–“é€²è¡Œæ™ºèƒ½è£œå……")
            
            # è¨ºæ–·ï¼šé¡¯ç¤ºç¼ºå¤± Bybit ä¸Šå¸‚æ™‚é–“çš„ symbols
            if by_loaded < len(tradable_symbols):
                missing_by = [s for s, times in self.listing_times.items() 
                            if times.get('bybit') is None]
                if len(missing_by) <= 10:
                    logger.debug(f"   Bybit ç¼ºå¤±ä¸Šå¸‚æ™‚é–“: {missing_by}")
                else:
                    logger.debug(f"   Bybit ç¼ºå¤±ä¸Šå¸‚æ™‚é–“: {len(missing_by)} å€‹ symbols")
            
        except Exception as e:
            logger.warning(f"ä¸Šå¸‚æ™‚é–“åŠ è¼‰æµç¨‹ç•°å¸¸: {e}ï¼Œå°‡ç¹¼çºŒä¸ä½¿ç”¨ä¸Šå¸‚æ™‚é–“é™åˆ¶")
    
    async def fetch_all_klines(
        self,
        tradable_symbols: List[str],
        config,
        exchanges: List[str] = ['binance', 'bybit']
    ) -> Dict[str, Dict[str, pd.DataFrame]]:
        """
        éåŒæ­¥ä¸¦è¡Œç²å–æ‰€æœ‰ K ç·š (å«é€²åº¦æ¢å’Œæ™ºèƒ½å¢é‡æ›´æ–°)
        æ”¯æŒæ ¹æ“šå¿«å–è¦†è“‹å’Œ symbol ä¸Šå¸‚æ™‚é–“è‡ªå‹•è£œå……ç¼ºå¤±æ•¸æ“š
        
        Args:
            tradable_symbols: å¯äº¤æ˜“çš„äº¤æ˜“å°åˆ—è¡¨
            config: å›æ¸¬é…ç½® (åŒ…å«æ™‚é–“ç¯„åœ)
            exchanges: äº¤æ˜“æ‰€åˆ—è¡¨
        
        Returns:
            {symbol: {exchange: DataFrame}}
        """
        start_ms, end_ms = config.get_time_range()
        
        print("\n" + "="*80)
        logger.info(f"ğŸš€ é–‹å§‹ä¸¦è¡Œç²å– {len(tradable_symbols)} å€‹ symbol çš„ K ç·š (æ™ºèƒ½å¢é‡æ¨¡å¼)")
        logger.info(f"ğŸ“… æ™‚é–“ç¯„åœ: {datetime.fromtimestamp(start_ms/1000)} åˆ° {datetime.fromtimestamp(end_ms/1000)}")
        logger.info(f"ğŸ’¾ å¿«å–ç›®éŒ„: {self.cache_dir}")
        print("="*80 + "\n")
        
        # Phase 0: åŠ è¼‰ä¸Šå¸‚æ™‚é–“
        await self._load_listing_times(tradable_symbols)
        
        tasks = []
        task_info = []
        
        # å»ºç«‹ä»»å‹™åˆ—è¡¨
        async with aiohttp.ClientSession() as session:
            for symbol in tradable_symbols:
                for exchange in exchanges:
                    # é¸æ“‡æ­£ç¢ºçš„ Semaphore
                    semaphore = self.bn_semaphore if exchange == 'binance' else self.by_semaphore
                    
                    task = self._fetch_with_semaphore(
                        symbol, exchange, start_ms, end_ms, session, semaphore
                    )
                    tasks.append(task)
                    task_info.append((symbol, exchange))
            
            # ä¸¦è¡ŒåŸ·è¡Œ (å¸¶é€²åº¦æ¢)
            total_tasks = len(tasks)
            print(f"ğŸ“Š ä¸¦è¡ŒåŸ·è¡Œ {total_tasks} å€‹ä»»å‹™ ({len(tradable_symbols)} symbols Ã— {len(exchanges)} exchanges)...")
            print("â³ é€²åº¦æ¢:\n")
            
            start_time = time.time()
            
            # ä½¿ç”¨ asyncio.gather åŸ·è¡Œï¼ŒåŒæ™‚ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦
            # å‰µå»ºé€²åº¦æ¢
            pbar = tqdm(total=total_tasks, desc="Kç·šä¸‹è¼‰é€²åº¦", unit="task")
            
            # å‰µå»ºåŒ…è£å¾Œçš„ä»»å‹™ï¼Œæ¯å€‹ä»»å‹™å®Œæˆå¾Œæ›´æ–°é€²åº¦æ¢
            async def task_with_progress(task):
                try:
                    result = await task
                    pbar.update(1)
                    return result
                except Exception as e:
                    pbar.update(1)
                    logger.error(f"ä»»å‹™ç•°å¸¸: {e}")
                    return None, None, None
            
            wrapped_tasks = [task_with_progress(task) for task in tasks]
            
            # åŸ·è¡Œæ‰€æœ‰ä»»å‹™ (æ”¹ç‚º return_exceptions=False è®“ç•°å¸¸è¢« task_with_progress çš„ try-except æ•æ‰)
            results = await asyncio.gather(*wrapped_tasks, return_exceptions=False)
            pbar.close()
            
            # æ•´ç†çµæœ
            klines_dict = {}
            success_count = 0
            none_count = 0
            error_count = 0
            
            for i, result in enumerate(results):
                if result is None:
                    none_count += 1
                    continue
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯ç•°å¸¸å°è±¡ï¼ˆé›–ç„¶ä¸æ‡‰è©²æœ‰ï¼Œä½†ä»¥é˜²è¬ä¸€ï¼‰
                if isinstance(result, Exception):
                    logger.error(f"çµæœ {i}: ç•°å¸¸å°è±¡ - {result}")
                    error_count += 1
                    continue
                
                if not isinstance(result, tuple) or len(result) != 3:
                    logger.warning(f"çµæœ {i}: æ ¼å¼ä¸å°ï¼Œtype={type(result)}, len={len(result) if isinstance(result, (list, tuple)) else 'N/A'}")
                    error_count += 1
                    continue
                
                symbol, exchange, df = result
                
                # ç„¡è«– df æ˜¯å¦ç‚º Noneï¼Œéƒ½è¦ä¿æŒ {symbol: {exchange: df}} çš„çµæ§‹
                if symbol not in klines_dict:
                    klines_dict[symbol] = {}
                
                klines_dict[symbol][exchange] = df
                
                if df is None:
                    none_count += 1
                else:
                    success_count += 1
            
            elapsed = time.time() - start_time
            print(f"\nâœ… å®Œæˆ")
            print(f"   æˆåŠŸ: {success_count}/{total_tasks}")
            print(f"   ç„¡æ•¸æ“š (None): {none_count}")
            print(f"   éŒ¯èª¤: {error_count}")
            print(f"   è€—æ™‚: {elapsed:.1f} ç§’ ({total_tasks/elapsed:.1f} tasks/sec)")
            print("="*80 + "\n")
            
            if not klines_dict:
                logger.warning("âš ï¸ è­¦å‘Š: klines_dict ç‚ºç©ºï¼Œæ²’æœ‰ K-line æ•¸æ“šè¢«åŠ è¼‰")
            else:
                logger.info(f"âœ“ åŠ è¼‰å®Œæˆ: {len(klines_dict)} å€‹ symbolsï¼Œ{success_count} å€‹æ•¸æ“šé›†")
            
            return klines_dict
